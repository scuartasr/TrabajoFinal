---
title: "Agresiones en la página de Instagram de Noticias Uno"
author:
  - Sofía Cuartas García
  - Simón Cuartas Rendón
  - Julián Alejandro Úsuga Ortiz
  - Deivid Zhang Figueroa
date: "Febrero de 2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r, warning = FALSE, include = FALSE}
library(tidyverse)
library(gridExtra)
library(knitr)
```


# Motivación y contexto

Durante la última década se ha visto un aumento vertiginoso en la cantidad de usuarios que tienen cuentas en las diferentes redes sociales disponibles en el mercado, permitiendo a las personas interactuar de nuevas maneras mediante estos espacios virtuales, en los cuales les resulta posible compartir momentos de sus vidas mediante fotografías o videos, así como encontrarse con otros usuarios para discutir puntos de vista sobre algún tema y  compartir noticias o contenidos de su interés.

No obstante, uno de los grandes problemas asociados a las redes sociales es la proliferación de agresiones y contenido de odio desde las más diversas fuentes, como lo pueden ser las noticias falsas, interacciones con usuarios con ideologías diferentes y comentarios a influencers (personas destacadas en las redes sociales) que no son de su agrado, entre otras.

Es por ello que nos motivamos a estudiar la cantidad total de comentarios con adjetivos peyorativos, emojis o cualquier otra representación o símbolo que evidencien de forma explícita la intención de agredir a una persona, grupo de personas, empresa, región geográfica o institución pública o privada, entre otras, que ocurren en la sección de comentarios de las publicaciones de Instagram del medio de comunicación Noticias Uno de Colombia. Pero además, se va a estudiar el promedio de comentarios peyorativos por publicación. Adicionalmente, se va a estimar la proporción de comentarios agresivos con el objetivo de mejorar las estimaciones (que son aquellas que carecen de un nombre real o una foto con una persona en ella).

# Parámetros a estimar

* **Total** de comentarios peyorativos en la sección de comentarios de las publicaciones de Instagram del medio de comunicación Noticias Uno de Colombia.
* **Promedio** de comentarios que se consideran negativos en la sección de comentarios de las publicaciones de Instagram del medio de comunicación Noticias Uno de Colombia. 
* **Proporción** de comentarios agresivos en la sección de comentarios de las publicaciones de Instagram del medio de comunicación Noticias Uno de Colombia.

# Aspectos importantes

* **Elemento.** Se considera como elemento a cada uno de los comentarios en las publicaciones de Instagram realizadas por el medio de comunicación radial Noticias Uno de Colombia. ``¿NO SERÍAN LAS PUBLICACIONES?`

* **Población.** La población está dada por la totalidad de comentarios realizados en las publicaciones de Instagram realizadas por el medio de comunicación radial Noticias Uno de Colombia.

* **Unidad de muestreo.** Para lograr la estimación de los parámetros de interés, se realizará el estudio considerando como unidad de muestreo a cada uno de los comentarios de las publicaciones de Instagram realizadas por el medio de comunicación radial Noticias Uno de Colombia, que con corte a las 4:00 p.m. del viernes 10 de diciembre de 2021 totalizaron 2,290 publicaciones. Se debe tener en cuenta, además, que para poder listar todas estas publicaciones se usó XXXX, el cual es una extensión de Chrome que se encarga de enlistar de forma automática todas las publicaciones de cualquier página de Instagram junto su enlace y la fecha de publicación de cada publicación, además de asignarle un número, el cual usamos como denominación para una de las publicaciones. Luego, para cada publicación, se tiene que esta exhibe de forma pública el número de comentarios que hay en esta y luego se los enlista en orden de aparición.

* **Tipo de muestreo.** Para abordar este trabajo se va a emplear el **muestreo por conglomerados**, para lo cual se toman los índices o ID de cada uno de los elementos muestrales, esto es, las fotos publicadas en la página de Instagram de *Noticias Uno* y se les organiza aleatoriamente para proseguir con una agrupación en grupos de quince publicaciones para poder construir los conglomerados, que serán quince en total, y a continuación se seleccionan de forma aleatoria veinte conglomerados para poder proceder con el muestreo. Todo lo anterior se realiza con ayuda de $\color{blue}{\textsf{R}}$.

```{r, include = FALSE}
set.seed(20220126)
# Aleatorización
indices = seq(1, 2295)

aleatorio = sample(indices,
                   size = 2295,
                   replace = FALSE)


# Matriz
auxiliar = matrix(aleatorio, ncol = 15, byrow = T)

# Conglomerados
conglomerados = as.data.frame(auxiliar)
save(conglomerados, file = "Conglomerados.RData") # ¿Guardar como .csv?
```

## Levantamiento de la información.

Para poder realizar el levantamiento de la información usando muestreo por conglomerados se apeló a una herramienta que se encarga de enumerar cada una de las publicaciones de la página de *Instagram* de *Noticias Uno*, incluyendo el enlace que dirige a cada publicación, la fecha en la que fue subida a la red social y un índice, dándole el primero a la publicación más reciente al momento de generar el listado, y el último a la primera publicación compartida por Noticias Uno en *Instagram*. De esta forma, teniendo en cuenta que se consideraron todas las aplicaciones que se publicaron hasta el sábado 11 de diciembre de 2021 a las 16:00 (4:00 p.m. GMT -5, es decir, hora de Bogotá), se tiene que el tamaño de la población, esto es, el número total de publicaciones en Instagram de Noticias Uno hasta dicha fecha es de 2,290 y se procede con la construcción de los conglomerados como se explicó antes

Con lo anterior presente, se generan con $\color{blue}{\textsf{R}}$ cuarenta números aleatorios para determinar cuáles van a ser los conglomerados a censar para recoger la información necesaria, los cuales se observan en la siguiente tabla

```{r}
set.seed(1037670103)
num.congl = seq(1, 153)
muestra = sort(sample(num.congl,
       size = 20,
       replace = FALSE))
```

```{r}
cong.muestreo = conglomerados[which(indices %in% muestra),]
write.csv(cong.muestreo, file = './muestreo.csv')
```


Para su levantamiento, entonces, cada uno de los cuatro integrantes asumió la recogida de datos de cinco conglomerados. Ahora bien, para el registro de los datos se creó un formulario de Microsoft Forums, el cual puede ser visto dando clic [aquí.](https://forms.office.com/Pages/ResponsePage.aspx?id=DQSIkWdsW0yxEjajBLZtrQAAAAAAAAAAAANAATOOFRZUMTU3SUY3SDgzVkdCU01OMVJMRFZKUlI4Vi4u) A continuación se muestran y se justifican cada uno de los elementos de este formulario.

* **Encuestador.** Registra cuál es la persona que está llenando la información.
* **Enlace.** Dirección web de la publicación.
* **ID.** Número de la publicación en el marco muestral.
* **Conglomerado.** ID del conglomerado asociado a la publicación censada. 
* **Fecha de publicación.** Fecha en la que fue subida la publicación a Instagram.
* **Número de comentarios.** Cantidad total de comentarios.
* **Número de comentarios con mensajes peyorativos.** Cantidad de comentarios en la publicación que tenían mensajes peyorativos, agresivos o violentos, entre otros.
* **Tema.** Temática con la que se asocia la publicación. Para esta pregunta existen cuatro posibilidades:
- Política nacional.
- Deportes.
- Entretenimiento.
- Otros.

Y una vez recogidos los datos para las cuarenta publicaciones asignadas aleatoriamente por R, se genera automáticamente un cuadro en Excel con las respuestas, que puede ser visualizado haciendo clic aquí `XXXX`.

```{r}
datos <- read.csv("Datos.csv", encoding = "UTF-8")
datos = na.omit(datos)
```

```{r}
datos = datos %>% 
  mutate(Fecha = as.Date(Fecha, format = "%d/%m/%Y")) %>% 
  rename(Encuestador = X.U.FEFF.Encuestador)
```

## Estudio piloto
`Bla, bla, bla`

Como se manifestó previamente, en este estudio se están considerando $N = `r nrow(conglomerados)`$, cada uno de $M = `r ncol(conglomerados)`$ unidades básicas. Así, se tiene que el número de unidades elementales en la población, es decir, el número publicaciones total a considerar es $M_0 = NM = `r nrow(conglomerados)*ncol(conglomerados)`$, que como se nota, son cinco unidades elementales más de las que realmente fueron consideras, esto porque se aleatorizaron los conglomerados con cinco unidades elementales imaginarias extras para poder asegurar que todos tuvieran tamaño uno. Ahora bien, para este estudio piloto se están considerando $n = 20$ conglomerados diferentes, seleccionados de manera aleatoria con ayuda de $\textsf{R}$, obteniendo que los que deben ser considerados son los siguientes:

`Insertar lista de conglomerados a estudiar (db conglomerados)`

Con esto, se va a considerar a $\mu_c$ como la media de comentarios peyorativos por conglomerados, $\mu$ la media de comentarios peyorativos en toda la población, es decir, en todas las publicaciones de *Noticias Uno* consideradas hasta la fecha de corte y $\tau$ el total. Además, sea $y_{ij}$ el número de comentarios peyorativos registrados en la *i-ésimo* publicación del *j-ésimo* conglomerado. Se sabe que los parámetros anteriores pueden ser calculados como sigue:

\begin{gather*}
\tau = \sum_{i=1}^{N} \tau_i = \sum_{i} \sum_{j} y_{ij} \\
\mu_c = \frac{\tau}{N} \\
\mu = \frac{\tau}{NM} = \frac{\tau}{M_0}
\end{gather*}

Así, vale la pena comenzar observando el resultado obtenido en cada uno de los conglomerados en la siguiente tabla:


```{r}
comen.c = datos %>% 
  group_by(as.factor(datos$Conglomerado)) %>% 
  summarise(Comentarios = sum(Comentarios)) %>% 
  rename(Conglomerado = `as.factor(datos$Conglomerado)`)
anon.c = datos %>% 
  group_by(as.factor(datos$Conglomerado)) %>% 
  summarise(Ofensivos = sum(Ofensivos)) %>% 
  rename(Conglomerado = `as.factor(datos$Conglomerado)`) %>% 
  rename(Anonimo = Ofensivos)
peyor.c = datos %>% 
  group_by(as.factor(datos$Conglomerado)) %>% 
  summarise(Peyorativos = sum(Peyorativos)) %>% 
  rename(Conglomerado = `as.factor(datos$Conglomerado)`)
```

```{r}
resumen = merge(comen.c, peyor.c, by = "Conglomerado")
resumen = merge(resumen, anon.c, by = "Conglomerado")
```

```{r}
kable(resumen, caption = "Resultados en los conglomerados del estudio piloto")
```

Definido lo anterior, se puede comenzar estimando la media poblacional por conglomerado $\mu_c$ mediante la media muestral por conglomerado $\hat{\mu}_c$, el cual, teniendo en cuenta que los conglomerados tienen igual tamaño, puede ser calculada como sigue:

$$\hat{\mu}_c = \frac{1}{n} \sum_{i=1}^{n} \tau_i = \bar{y}_c$$

Así que al emplear la información de la tabla uno, se tiene que la media muestral por conglomerado es:

```{r}
suma.pey.c = sum(resumen[, 3])
congs = nrow(resumen)
est.m.c = suma.pey.c / congs
```

$$\hat{\mu}_c = \frac{4 + 16 + \ \ldots \ + 7 + 10}{`r nrow(resumen)`} = \frac{`r suma.pey.c`}{`r nrow(resumen)`} = `r round(suma.pey.c/nrow(resumen), 4)`$$

Lo que quiere decir que en cada conglomerado hay un promedio de `r round(est.m.c, 2)` comentarios peyorativos. Y con esto se puede estimar la cantidad promedio de comentarios peyorativos por unidad elemental como sigue:

```{r}
M = 15
est.m = est.m.c/M
```


$$\hat{\mu} = \frac{\hat{\mu}_c}{M} = `r round(est.m, 4)`$$
Es decir, existe un promedio de `r round(est.m, 2)` comentarios peyorativos, agresivos o irrespetuosos en cada publicación de *Instagram* de *Noticias Uno*. Además, el total de comentarios peyorativos puede ser estimado como sigue:

```{r}
options(scipen=999)
N = 153
Mo = M * N
est.t.c = N * est.m.c
```



$$\hat{\tau}_c = N \hat{\mu}_c = `r Mo` \times `r round(est.m.c, 4)` \approx `r round(est.t.c, 4)`$$
De manera que se estima que se han dejado un total de `r round(est.t.c)` comentarios peyorativos en todas las publicaciones de *Instagram* de *Noticias Uno* que habían si subidas a la red social hasta el once de diciembre de 2021.

Ahora bien, con el objetivo de determinar cuál es el número de conglomerados que deben ser estudiados en el muestreo final, es necesario obtener la varianza estimada de los estimadores expuestos previamente, lo cual se consigue con las siguientes ecuaciones:

\begin{gather*}
\widehat{Var} [\hat{\tau}_c] = \frac{N(N-n)}{N} \sum_{i = 1}^{n} \frac{(\tau_i - \hat{\mu}_c)^2}{n-1} \\
\widehat{Var} [\hat{\mu}] = \frac{1}{M_0^2} \widehat{Var} [\hat{\tau}_c]
\end{gather*}

```{r}
# Total
n.ep = 20
totales.cong = as.vector(resumen[, 3])
paso1 = (totales.cong - est.m.c)^2
paso2 = paso1 / (n.ep - 1)
est.var.tc = sum(paso2) * N * (N - n.ep) / n.ep

# Media
est.var.m = est.var.tc / (Mo^2)

```

Y al realizar los respectivos cálculos se tiene que los estimadores de dichas varianzas son:

\begin{gather*}
\widehat{Var} [\hat{\tau}_c] = `r round(est.var.tc, 4)` \\\
\widehat{Var} [\hat{\mu}] = `r round(est.var.m, 4)`
\end{gather*}

Y con estos cálculos se tienen la mayoría de elementos necesarios para poder determinar el número de conglomerados que deben ser finalmente considerados.

### Tamaño de muestra para la media poblacional

Para calcular el número de conglomerados $n_{\mu}$ necesarios para estimar con la muestra a la media de comentarios peyorativos en todas las publicaciones es necesario emplear la siguiente ecuación:

$$n = \frac{n_0}{1 + \frac{n_0}{N}}, \ \ \ \ \ n_0 = \frac{Z_{\alpha/2}^2 S^2_{con}}{B_{\mu}^2 M^2}$$

Así, es necesario calcular el valor de $S^2_{con}$, el cual puede ser calculado como sigue:

$$S^2_{con} = \sum_{i = 1}^{n} \frac{(\tau_i - \hat{\mu}_c)^2}{n-1}$$

```{r}
alfa = 0.10
z = qnorm((alfa/2), mean  = 0, sd = 1, lower.tail = FALSE)
epsilon = 0.15
Bm = epsilon * est.m
```


Para el cual, al realizar los cálculos con $\textsf{R}$, se obtiene que $S^2_{con} = `r round(sum(paso2), 4)`$. Además, considerando que se va a emplear un nivel de confianza del 90 % ($\alpha = 0.10$) (con lo que $Z_{0.05/2} = `r round(z, 4)`$) y un límite en el error de la estimación relativo del 10%, es decir, $\epsilon = 0.15$, de manera que el límite en el error de estimación $B_{\mu}$ es: $B_{\mu} = 0.10 \times `r round(est.m, 4)` = `r round(Bm, 4)`$. Así, se tiene que el número de conglomerados a muestrear si estos fuesen infinitos es:

```{r}
n0.m = ((z^2) * sum(paso2))/((Bm * M))^2
```


```{r}
n.m = n0.m / (1 + n0.m/N)
```

De manera que, al realizar la corrección por población finita y realizar las cuentas, se tiene que el número de conglomerados a muestrear es $`r ceiling(n.m)`$

### Tamaño de muestra para el total

Para encontrar el tamaño de muestra necesario para estimar el total poblacional es necesario emplear la siguiente ecuación:

$$n = \frac{N S_{con}^2}{ND + S_{con}^2}, \ \ \ \ D = \frac{B_{\tau}^2}{Z_{\alpha/2}^2 N^2}$$

```{r}
Bt = epsilon * est.t.c
```


De manera que para poder emplear esta ecuación es necesario comenzar encontrando el límite en el error de estimación, que, considerando que este está dado por $B_{\tau} = \epsilon \times \hat{\tau}}$, de manera que se obtiene que $B_{\tau} = `r round(Bt, 4)`$. Luego, realizando las cuentas indicadas por las dos ecuaciones anteriores se tiene que:

```{r}
D = (Bt / (z * N))^2
n.t = (N * sum(paso2))/(N*D + sum(paso2))
```

$$D = `r round(D, 4)`, \ \ \ \ n_{\tau} \approx `r round(n.t, 4)` $$

Y como se ve el resultado es el mismo que para el tamaño de la muestra de la media poblacional, es decir, que se deben muestrear un total de `r ceiling(n.t, 4)` conglomerados.